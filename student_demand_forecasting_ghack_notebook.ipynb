{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aa31889-06a5-474d-ad5d-62d19f7a2cc8",
   "metadata": {},
   "source": [
    "## AutoML tabular forecasting model for batch prediction\n",
    "\n",
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to create an `AutoML` tabular forecasting model from a Python script, and then do a batch prediction using the Vertex AI SDK. You can alternatively create and deploy models using the `gcloud` command-line tool or online using the Cloud Console.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `AutoML Training`\n",
    "- `Vertex AI Batch Prediction`\n",
    "- `Vertex AI Model` resource\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create a `Vertex AI Dataset` resource.\n",
    "- Train an `AutoML` tabular forecasting `Model` resource.\n",
    "- Obtain the evaluation metrics for the `Model` resource.\n",
    "- Make a batch prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff221e7c-a241-473e-badd-d65e25928ab7",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a010dfa2-a698-4b53-853e-99f32a60a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068fa850-0dac-47d3-88d8-24585447fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "from google.cloud import bigquery\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f5407-2ea4-43d8-8033-d4d88a5b000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9740f63-6b65-4931-bdcc-334057940f6f",
   "metadata": {},
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7ff2d1-4a90-4a35-98c8-7f00c8494d1f",
   "metadata": {},
   "source": [
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. It is recommended that you choose the region closest to you. For this GHack we'll use us-central1.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a44ef1-53ad-4a9c-84f0-130f32fab390",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544be43c-d4c1-43fa-a748-f8f8c5fdf9d5",
   "metadata": {},
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643e451-6f9f-41a0-a9ca-7d5a92bcb69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42e2439-9b66-4b06-9e9f-0c5a0043a587",
   "metadata": {},
   "source": [
    "#### UUID\n",
    "\n",
    "To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this GHack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52225d0-b0a3-41b9-bb6f-2646668db2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3523f095-018d-48b7-b7a5-dcff75549bd5",
   "metadata": {},
   "source": [
    "# GHack\n",
    "\n",
    "Now you are ready to start creating your own AutoML tabular forecasting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c94139-eae9-49de-8e82-72e8be355111",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea9e0a-467b-47d7-9e28-bd7c32b6a0de",
   "metadata": {},
   "source": [
    "#### Location of BigQuery training data.\n",
    "\n",
    "Now set the variable `TRAINING_DATASET_BQ_PATH` to the location of the BigQuery table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975956f2-9175-4baa-9961-eb9a86f6da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATASET_BQ_PATH = (\n",
    "    \"bq://bigquery-public-data:iowa_liquor_sales_forecasting.2020_sales_train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a1e750-a3e3-4068-8d71-d84f90872c5e",
   "metadata": {},
   "source": [
    "### Create the Dataset\n",
    "\n",
    "Next, create the `Dataset` resource using the `create` method for the `TimeSeriesDataset` class, which takes the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Dataset` resource.\n",
    "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
    "- `bq_source`: Alternatively, import data items from a BigQuery table into the `Dataset` resource.\n",
    "\n",
    "This operation may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243504e-8573-4694-bb23-326f4bf56c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aiplatform.TimeSeriesDataset.create(\n",
    "    display_name=\"iowa_liquor_sales_train\" + \"_\" + UUID,\n",
    "    bq_source=[TRAINING_DATASET_BQ_PATH],\n",
    ")\n",
    "\n",
    "time_column = \"date\"\n",
    "time_series_identifier_column = \"store_name\"\n",
    "target_column = \"sale_dollars\"\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90a822-ad1f-4b70-8a57-b4545d8689a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_SPECS = {\n",
    "    time_column: \"timestamp\",\n",
    "    target_column: \"numeric\",\n",
    "    \"city\": \"categorical\",\n",
    "    \"zip_code\": \"categorical\",\n",
    "    \"county\": \"categorical\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f7d94-d2ec-45b2-aabb-f543f994154f",
   "metadata": {},
   "source": [
    "### Create and run training job\n",
    "\n",
    "To train an AutoML model, you perform two steps: 1) create a training job, and 2) run the job.\n",
    "\n",
    "#### Create training job\n",
    "\n",
    "An AutoML training job is created with the `AutoMLForecastingTrainingJob` class, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `TrainingJob` resource.\n",
    "- `column_transformations`: (Optional): Transformations to apply to the input columns\n",
    "- `optimization_objective`: The optimization objective to minimize or maximize. Some examples:\n",
    "    - `minimize-rmse`\n",
    "    - `minimize-mae`\n",
    "    - `minimize-rmsle`\n",
    "    - `minimize-quantile-loss`\n",
    "    \n",
    "\n",
    "The instantiated object is the job for the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a4a93-9ccf-4e69-8cea-68aab64cb101",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DISPLAY_NAME = f\"iowa-liquor-sales-forecast-model_{UUID}\"\n",
    "\n",
    "training_job = #['Fill in here']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab46390-4237-4f2f-9bca-ad47bcd8b825",
   "metadata": {},
   "source": [
    "#### Run the training pipeline\n",
    "\n",
    "Next, you start the training job by invoking the method `run`, with the following parameters:\n",
    "\n",
    "- `dataset`: The `Dataset` resource to train the model.\n",
    "- `model_display_name`: The human readable name for the trained model.\n",
    "- `training_fraction_split`: The percentage of the dataset to use for training.\n",
    "- `test_fraction_split`: The percentage of the dataset to use for test (holdout data).\n",
    "- `target_column`: The name of the column to train as the label.\n",
    "- `budget_milli_node_hours`: (optional) Maximum training time specified in unit of millihours (1000 = hour).\n",
    "- `time_column`: Time-series column for the forecast model.\n",
    "- `time_series_identifier_column`: ID column for the time-series column.\n",
    "\n",
    "The `run` method when completed returns the `Model` resource.\n",
    "\n",
    "The execution of the training pipeline will take up to 2-3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b3e6b4-a520-4403-9313-040fb3094aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = #['Fill in here']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398544e3-d34e-4223-b8ff-c5ebd7692a3e",
   "metadata": {},
   "source": [
    "## Review model evaluation scores\n",
    "\n",
    "After your model training has finished, you can review the evaluation scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af814a92-eae2-44f1-a744-a80ec83202f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#['Print your model evaluation results']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb05bd-0c9a-411e-9b9d-2cb364758c48",
   "metadata": {},
   "source": [
    "## Send a batch prediction request\n",
    "\n",
    "Send a batch prediction to your deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211a7fea-7bcf-44af-ab7b-0607c5ddbcb4",
   "metadata": {},
   "source": [
    "### Make the batch prediction request\n",
    "\n",
    "Now that your Model resource is trained, you can make a batch prediction by invoking the batch_predict() method using a BigQuery source and destination, with the following parameters:\n",
    "\n",
    "- `job_display_name`: The human readable name for the batch prediction job.\n",
    "- `bigquery_source`: BigQuery URI to a table, up to 2000 characters long. For example: `bq://projectId.bqDatasetId.bqTableId`\n",
    "- `bigquery_destination_prefix`: The BigQuery dataset or table for storing the batch prediction resuls.\n",
    "- `instances_format`: The format for the input instances. Since a BigQuery source is used here, this should be set to `bigquery`.\n",
    "- `predictions_format`: The format for the output predictions, `bigquery` is used here to output to a BigQuery table.\n",
    "- `generate_explanations`: Set to `True` to generate explanations.\n",
    "- `sync`: If set to True, the call will block while waiting for the asynchronous batch job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79effb54-cb6c-4c7f-bd76-997ea54cd731",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predict_bq_output_dataset_name = f\"iowa_liquor_sales_predictions_{UUID}\"\n",
    "batch_predict_bq_output_dataset_path = \"{}.{}\".format(\n",
    "    PROJECT_ID, batch_predict_bq_output_dataset_name\n",
    ")\n",
    "batch_predict_bq_output_uri_prefix = \"bq://{}.{}\".format(\n",
    "    PROJECT_ID, batch_predict_bq_output_dataset_name\n",
    ")\n",
    "# Must be the same region as batch_predict_bq_input_uri\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "bq_dataset = bigquery.Dataset(batch_predict_bq_output_dataset_path)\n",
    "dataset_region = \"US\"  # @param {type : \"string\"}\n",
    "bq_dataset.location = dataset_region\n",
    "bq_dataset = client.create_dataset(bq_dataset)\n",
    "print(\n",
    "    \"Created bigquery dataset {} in {}\".format(\n",
    "        batch_predict_bq_output_dataset_path, dataset_region\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32e9798-8070-47ba-af68-7d97596426de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PREDICTION_DATASET_BQ_PATH = (\n",
    "    \"bq://bigquery-public-data:iowa_liquor_sales_forecasting.2021_sales_predict\"\n",
    ")\n",
    "\n",
    "batch_prediction_job = #['Create batch prediction']\n",
    "\n",
    "print(batch_prediction_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c831d24a-8c51-4974-97b1-6d531d987606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#['Print batch prediction results']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf109e0-9e93-4f8d-8578-302a9aa1ad57",
   "metadata": {},
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Dataset\n",
    "- AutoML Training Job\n",
    "- Model\n",
    "- Batch Prediction Job\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc05d27-3e75-4f34-b6ce-4adb195fe065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete dataset\n",
    "dataset.delete()\n",
    "\n",
    "# Training job\n",
    "training_job.delete()\n",
    "\n",
    "# Delete model\n",
    "model.delete()\n",
    "\n",
    "# Delete batch prediction job\n",
    "batch_prediction_job.delete()\n",
    "\n",
    "# Set this to true only if you'd like to delete your bucket\n",
    "delete_bucket = False\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
